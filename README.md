# LLM can Fool Itself: A Prompt-Based Adversarial Attack
This is the source code for the paper "LLM can Fool Itself: A Prompt-Based Adversarial Attack",
<br> Xilie Xu (NUS), Keyi Kong (SDU), Ning Liu (SDU), Lizhen Cui (SDU), Di Wang (KAUST), Jingfeng Zhang (University of Auckland/RIKEN-AIP), Mohan Kankanhalli (NUS).
<br> [[PDF]]() [[Project Page]](https://godxuxilie.github.io/project_page/prompt_attack)

introduction picture real


## Environment


## Let's Attack the LLM via PromptAttack!

Three parts

###
###
### AO


### Script


## Adversarial Examples
 some samples

## BibTeX
```

```

## Contact
Please drop an e-mail to xuxilie@comp.nus.edu.sg if you have any enquiry.
